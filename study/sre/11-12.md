# 11. 비상대기 Being On-Call
비상대기 - 서비스의 신뢰성과 가용성을 위해 반드시 수행해야할 임무

~~구글의 SRE들도 비상대기로부터 자유롭진 못하..~~

> 구글의 SRE 들이 수년에 걸쳐 개발한 비상 대기 업무 수행 방안의 기본적인 원리 소개

## 소개
- SRE 들이 서비스를 위한 비상 대기 업무를 수행
- 단순 운영이 아닌, 문제를 해결하기 위한 **엔지니어링적 접근법**을 강조
- 순수 운영 업무는 최대 50%의 시간으로 제한 
- 50%의 시간은 서비스 개선 및 자동화를 통해 업무 환경을 개선할 수 있도록 엔지니어링 프로젝트에 투입

## 비상 대기 엔지니어의 삶
> 비상 대기 엔지니어는 프로덕션 시스템의 보호자로, 
>
> 팀에 영향을 미치는 장애를 관리하고 프로덕션 환경의 변경을 추진 및 진단하는 등 할애된 운영 업무를 수행한다.

- 비상 대기 시에 엔지니어는 수 분 이내 운영작업을 수행해야함.
- 사전에 약속된 장애 대응 시간
	- 서비스중요도에 따라 5분 ~ 30분
	- 사용자 노출 서비스의 경우 분기별 가용성 99.99%를 확보 (다운타임 13분)
	- 분 단위 장애대응이 필요(SLO 낮은 경우에는 10분 단위로도 정의함)
- 이메일, SMS, 자동 전화, 모바일 앱등의 알림 시스템 운영
	- 우선순위가 낮은 알림 등은 업무 시간 중 처리
- 비상 대기조 - 보조 대기조 운영 (a primary and a secondary on-call rotation)
	- 비상 대기조가 알람 수신을 놓친 경우를 대비한 보조 대기조 운영 
	- 비상 대기조는 오로지 장애 알림처리만, 보조 대기조는 프로덕션 업무 수행

## 비상 대기 업무의 균형 맞추기
> 비상 대기 업무에 편성 되는 경우, 업무의 양과 품질에 제약이 있다.
>
> 비상 대기 업무의 양은 엔지니어가 비상 대기 업무에 할애한 시간의 백분율로 계산한다.
>
> 비상 대기 품질은 비상 대기 업무 기간동안 발생한 장애의 수로 계산 될수 있다.

### 업무 양의 균형

- 50% 엔지니어링 프로젝트, 25% 비상 대기, 25% 운영 업무
- 25% 비상 대기 -> 24/7 대기하는 비상 대기조 운영이 가능 (계산법 ‘-‘??)
	- 단일사이트 : 두사람이 주/보조 로 대기. 8명. 4주에 1주는 비상대기.
	- 다중사이트 : 6명
		- 야간 교대를 하지 않는다.
		- 비상 대기 업무에 참여하는 엔지니어의 수를 제한 -> 엔지니어들의 시스템에 대한 관심 지속
		- 의사소통과 협업에 더 많은 비용을 소모

### 품질의 균형

- 비상 대기 시 장애의 주요 원인 분석과 개선, 포스트 모텀 작성, 버그 수정과 같은 후속 작업등으로 평균 6시간 소요. 
- 비상대기 교대는 매 12시간 마다 이루어지므로 이 부석에 따르면 하루에 처리할수 있는 최대 장애는 2개.
- 어떤 컴포넌트나 이슈가 매일 호출을 발송한다면 (평균 장애 수/일 수 > 1) 특정 시점에 다른 어딘가에 장애가 발생하여 평소보다 많은 장애가 발생.

> 한계값이 분기당 한번 정도와 같이 일시적으로 초과 된다면 이를 보정하기 위한 측정을 도입하여 `운영 부하`를 지속 가능한 상태로 유지해야함.

### 보상
- 구글의 경우 대체휴무나 전체 연봉에 대한 일정 비율의 현금보상을 제공
- 보상의 상한이 존재하면 특정 개인이 비상 대기 업무에 투입되는 시간을 제어

> 보상 정책을 통해 비상 대기 업무에 투입되는 부분에 대한 동기를 부여
>
> 비상 대기 업무의 균형적인 분산과 피로 혹은 프로젝트 업무 수행을 위한 시간 부족등 초과 비상대기 업무로 인한 잠재적 부작용 해소

## 안전에 대해 고려하기

> 사용자가 직접 사용하며, 수익과 관련된 시스템이나 이런 시스템들을 지속적으로 운영하기 위해 필요한 인프라에 대한 관리 책임을 진다는 것을 의미

- 복잡한 시스템의 장애를 처리할때는 합리적, 집중적, 그리고 계획적이며 경험에 기반한 선택이 효과적
- 엔지니어들이 자신을 제어할수 있도록 비상대기와 관련된 스트레스를 줄여주는 것이 중요
    - 서비스의 중요도 및 영향도와 잠재적 장애의 결과는 비상 대기를 수행중인 엔지니어에게 압박
    - 코르티솔, 코르티코트로핀 같은 스트레스 호르몬이 인지적 행위에 부정적인 영향을 미쳐 부적절한 의사결정
- 스트레스 호르몬의 영향을 받지 말고 무계획적으로 대응해 보자.
    - -> 시행착오를 통한 자기 학습 (~~몸빵…~~) -> 
    - 동일 장애 알림 발생 시, 이전 대응 경험에 의해 같은 원인이라 치부하여 쉽게 생각하게 됨.
        - 직감(경험)에 기반한 신속한 대응이 미덕이나, 명확한 데이터에 근거한 지원으로 이어지지 않을 수 있음. -> 시간낭비 가능성

> 신속한 대응 또한 습관에 의해 좌우
>
> 자신의 추측을 심도있게 확인하는 동시에 좀 더 합리적인 의사 결정을 위해 
>
> 충분한 데이터를 활용할 수 있는 시점에 적절한 속도로 차근차근 단계를 밟아가는 완벽한 균형이 필요

<비상대기 업무에 대한 부담을 줄이는 자원들>
- 분명한 장애 전파 경로 Clear escalation paths
	- 시스템 개발 팀이 비상대기에 투입되며, 필요하다면 다른팀에 장애를 전파 할 수 있다.
- 잘 정의된 장애 관리 프로세스 Well-defined incident-management procedures
	- 장애의 수준이 복잡하고 장애해결에 소요시간이 불분명한 경우 장애 관리 절차를 따르는 것이 좋다.
	- 구글에서는 장애 절차를 웹 기반 도구를 통해 역할을 넘겨주거나 상태변화를 녹화 공유 장애 관리가 자동화 되어있다.
- 비난 없는 포스트모텀 문화 A blameless postmortem culture
	- 언제 무엇이 잘못 되고, 어떤 부분이 제대로 동작했는지 판단해서 향후에 동일한 에러가 반복하지 않도록 기록해야한다.
	- 사람이 아닌 사건에 집중하여 기술
	- 프로덕션환경의 장애를 시스템적으로 분석
	- 문제를 정의하여 자동화 할 수 있는 부분을 선별

## 부적절한 운영 부하에서 벗어나기
### 운영부하

이상적으로는 운영 업무 부담의 증가에 대한 증상을 측정할 수 있어서 그 목표치가 정량화 될 수 있어야 한다. 

(티켓Task 5개 미만, 비상 대기 동안 호출 알림 2회 미만 )

1. 모니터링 오설정

호출 알림은 서비스의 SLO를 위협하는 증상이 발생하는 경우 발송.

우선순위가 낮은 알림이 빈번한 경우, 생산성 저하 및 알림을 간과하게 되어 심각한 알림을 놓칠 가능성

우선순위 낮은 중복알림 비활성화

2. SRE 외부의 문제

애플리케이션 개발자의 작업과정에 시스템의 신뢰성에 문제 발생

해당 개발자와의 협업을 통해 시스템 향상을 위한 공통 목표 설정

…극한의 상황시 SRE가 개발팀에게 한시적으로 비상 대기에 집중해줄 것을 요청
	- 여러 부분에 영향을 미치는 복잡한 변경이나 아키텍처적인 변경을 적용 시, 시스템 안정화를 위해 요청

## 뜻밖의 적: 운영 업무 부족
- 시스템이 이상할만큼 문제가 없어서 비상대기 투입 빈도가 낮아짐 
	- -> 프로덕션 환경에 대한 자신감 하락 (또는 부족)
	- -> SRE의 지식과 프로덕션 환경의 차이를 장애가 발생하기전엔 알 수 없음
- 모든 SRE를 적절하게 비상대기 투입, 프로덕션 환경에 노출 -> 장애를 접하며, 서비스에 대한 문제 해결 능력과 지식을 갈고 닦을 기회를 얻을 수 있다..
- 구글은 연간 장애 복구 대회 개최(Disaster Recovery Training, DiRT) 



# 12. 효과적인 장애 조치 Effective Troubleshooting

> 단순히 시스템의 동작을 이해한다고 해서 전문가가 될 수 있는 것은 아니다.
> 
> 전문성이라는 것은 시스템이 동작하지 않는 이유에 대해 연구하는 과정에서 얻어지는 것이다.
> 
> -브라이언 레드맨

- 장애조치를 어떻게 해야하는지 설명하는 것은 자전거 타는 방법을 설명하는 것처럼 어렵다.

## 이론
- 가설귀납법적 방법
	- 시스템 관찰에 대한 결과와 시스템의 행동에 대한 이해를 기반으로 장애 원인에 대한 가설을 세우고 가설을 시험

![](https://landing.google.com/sre/book/images/srle-1201.jpg)

1. 시스템 측정 데이터와 로그를 통해 현재 상태 파악
2. 시스템 구현, 동작 방식 과 장애 복구 지식을 결합하여 원인을 규명
3. 가설을 테스트 (시스템의 상태를 이론과 비교하여 정황 분석, 통제된 방법으로 시스템을 수정 및 관찰)
4. 장애 재발을 방지하기 위해 장애 조치후 포스트모텀 문서를 작성

### 일반적인 문제
- 시스템에대한 깊은 이해가 부족하여 비효율적인 장애 조치 절차로 흘러감
	- 장애 등급 선정, 분석 및 진단 단계에 영향
- 관련이 없는 증상을 들여다보거나 시스템의 지표의 의미를 잘못이해하는 경우
- 시스템의 변경이나 입력 값 혹은 환경에 대한 잘못된 이해는 안전하고 효과적인 가설의 검증에 방해
- 장애 원인에 대한 가능성이 희박한 가설을 세우거나 과거 발생한 문제의 원인과 결부시켜 발생한 문제가 다시 발생할 것이라 결부해버리는 행위
- 우연히 발생했거나 동일한 원인에 의해 발생한 관련 현상들을 계속해서 쫓아다니는 행위

## 실전에 들어가보자
## 문제 보고
- 실제로 기대한 동작은 무엇인지, 그리고 현재 어떻게 동작하고 있는지, 문제가 되는 동작을 어떻게 재현할수 있는지 설명
- 버그 추적 시스템과 같이 검색이 가능한 위치에 저장
- 문제를 특정인에게 직접 보고하는 것은 지양

## 문제의 우선순위 판단
- 문제의 영향도에 따라 적절히 대처 방법을 선택
- `우선 시스템이 가능한 정삭적으로 동작하게 만드는 것이 목표`
	-  다른 클러스터에 트래픽을 전환, 연쇄적인 장애를 피하기 위해 트래픽 감소, 부하를 줄이기 위해 서브시스템 비활성화

	- 데이터 복구가 불가능할정도로 손상을 입었다면 손실방지를 위해 시스템 중단
- 파일럿은 긴급상황에서 자신의 최우선 과제는 비행기를 계속 비행하게하는것. 비행기와 그외 모든것들을 안전하게 착륙

## 문제를 관찰하기
- 이상적인 경우 모니터링시스템이 모든 지표를 기록
- 로그를 통해 각 동작에 대한 정보와 시스템의 상태를 살펴보며 특정 시점에 작업한 프로세스에 대한 이해
- 대퍼를 이용하여 전체 스택에 걸쳐 요청을 추적하는것은 분산시스템이 어떻게 동작하는지 이해할수 있는 방법
- 상태를 외부에 노출 하는 방법 
	- 최근 발수신 RPC 를 보여주는 종단점이 있음. 아키텍처 다이어그램이 없어도 서버 사이의 통신 수행을 이해. 
	- 설정을 보여주거나 데이터를 확인할 수 있는 종담점을 제공

## 진단
- 시스템 진단 기법을 사용하면 시스템에 전반적인 이해가 부족해도 문제 원인 파악을 할수 있다.

### 단순화하기와 범위를 좁히기
- 시스템 내의 컴포넌트 들은 잘 정의된 인터페이스와 더불어 입력 데이터를 출력데이터로 변환할수 있어야 함
- 컴포넌트 사이의 데이터 흐름을 확인하여 올바른 동작 여부를 확인
- 테스트 데이터를 통해 의도적 에러를 유발하기도 함
- 시스템을 각 계층별로 분할 정복기법은 범용적으로 해결책을 찾기위한 유용한 수단
	- 소규모의경우, 다계층의 컴포넌트 스택을 순차적으로 테스트를 진행하며 각 컴포넌트가 제대로 동작하는지 확인
	- 시스템이 거대한 경우, 시스템을 반으로 나누어 컴포넌트 사이의 통신 경로를 확인. 나머지 반쪽에 대해서 문제확인 수행. 

### 무엇이, 어디서, 왜를 고민하기
- 오동작 시스템은 우리가 원하는 동작 이외의 다른 동작을 계속적 실행
- 시스템의 동작을 확인 -> 왜 수행하는지 -> 자원을 어디서 사용하는지 -> 결과가 어디로 전달

### 가장 마지막으로 수정된 부분에 주목

> 시스템의 항상성. 올바르게 동작하는 컴퓨터 시스템은 설정의 변경이나 서비스 부하의 종류가 바뀌는 외부요인이 발생하기 전까지는 계속해서 동작 
> 
> -> 그래서 무엇이 잘못되고 있는지 파악하기 좋은 시작은 **최근 변경 지점**


**<새로운 버전 배포 시작 시점에서 완료까지 에러율>**
![](https://landing.google.com/sre/book/images/srle-1202.jpg)

### 서비스에 특화된 진단
- 특정 서비스를 분석하는데 도움이 되는 도구와 시스템을 개발하는 것이 좋다.
- 구글의 SRE들은 대부분의 시간을 이런 도구들을 개발하는데 사용
- 대부분의 서비스와 팀 간 공통성을 확인함으로써 중복된 노력을 제거하기위해 시스템 특화적으로 개발

## 테스트와 조치
- 가능한 원인을 파악 후 테스트를 통한 근본 원인 색출
- 코드의 흐름을 단계별로 모방해보며 문제점 색출

<예시>
- 애플리케이션 서버와 데이베이스 서버간 접속 장애 발생 시 가설검증
	- 동일인증정보로 DB 접근(DB 연결 거부), DB ping(네트워크 문제)

### 테스트 구상의 주의할 점

- 테스트는 상호 배타적이어서 가설의 어느 한 집합을 검증함으로써 다른 가설의 가능성이 없음

- 가장 명확한 것을 최우선으로 고려
	- 가능성이 큰 테스트부터 수행
	- 테스트로 인한 시스템의 장애 발생 가능성 위험을 고려

- 혼란 요소로 인한 특정 실험이 잘못된 결과를 도출할 가능성 고려
	- 방화벽 정책이 특정 IP 요청에만 응답하도록 적용 -> 서버내에선 문제가 없지만 외부 사용자에게는 실패

- 적극적인 테스트가 나중에 실행할 테스트 결과에 부작용을 초래
	- 테스트를 위한 프로세스가 cpu 점유를 과하게 설정하면 경쟁상태 발생 가능성 증가
	- 과한 양의 로그를 기록하도록 설정하여 지연 응답 문제 발생
	- 이런 경우 문제해결에 대한 정확한 판단이 어려움

- 일부 설득력이 떨어지는 테스트
	- 정기적이면서 반복가능한 형태로 경쟁상태나 데드락 상황을 만들기는 어려움
	- 문제의 원인으로 규명하기에 불확실한 증거임에도 만족할수 밖에 없음..

- 테스트 수행 여부 및 결과를 기록
	- 복잡하고 긴 시간을 소요하는 테스트는 **정확하게 어떤 현상이 발생** 하였는지 기록
	- 같은 과정을 반복하는 상황 제거
	- 시스템 원복에 도움이 됨
 

# 부정적인 결과의 마법
> `부정적인`결과는 기대한 효과가 나타나지 않은 경우에 대한 **경험의 산물**
> 계획대로 되지 않은 모든 실험

### 부정적인 결과는 무시해서도 안되고 평가절하해서도 안된다.

- 잘못에 대한 인식 그 자체에 가치

### 부정적인 결과로 끝난 실험 역시 결론이다.
- 결과를 통해 프로덕션 환경이나 디자인 공간 혹은 기존 시스템 성능 한계등을 확인할 수 있음
- 추후 그 실험 결과 문서를 통해 부정적인 결과를 초래할 시스템 디자인에 대한 평가를 빠르게 할 수 있음
- 마이크로벤치마크와 문서화된 안티패턴들, 그리고 프로젝트 포스트모텀은 이 범주에 해당
- 싫험을 계획할 때는 도출 가능한 부정적인 결과에 고려해야 함 (의미가 확실한 부정적 결과는 도움이 됨)

### 도구와 방법은 실험의 결과와는 무관하며 향후의 작업에 대한 단서가된다.
- 벤치마킹 도구들과 부하테스트 도구를 통해 기대에 미치지 못하는 실험을 긍정적인 방향으로 활용
- 아파치 벤치 같은 웹서버 부하테스트 도구들을 사용하며, 장기적으로 어렵고 세부적인 작업 내역을 바탕으로 많은 장점을 얻음
-> 누적된 툴/스크립트 사용 경험이 다음 애플리케이션 작업에 도움이 됨

### 부정적인 결과를 공표하는 것은 업계의 데이터 주도 성향을 증진시킨다

- 부정적인 결과와 통계적으로 그다지 의미가 없는 데이터에 대한 자세한 기록은 
- 우리가 가진 지표에 대한 편견을 해소하고 다른 사람들로 하여금 측정된 데이터에 근거해 불확실성을 이해하는 좋은 예시
- 결과를 공유하여 다른 이들도 같은 방법을 취하게끔 유도할 수 있으며, 업계의 모든 이들이 이를 일괄적으로 학습하게 할 수 있음
- 프로덕션 환경의 안정성에 커다란 긍정적 효과

### 자신의 결과를 공표하자
- 자신의 실험 결과를 공개하여 유사한 실험에 관심이 있는 사람에게 정보를 제공
- 자신이 배제한 디자인, 알고리즘, 팀의 업무 흐름에 대해 공유
- 부정적인 결과는 신중하게 위험을 받아들이는 과정의 일부
- **실패를 언급하지 않은 모든 디자인 문서, 성능 리뷰, 에세이 들은 의심하라.**
	- 많은 생략, 면밀히 검토하지 않았을 가능성.

## 처방
- 원인이 실제로 문제의 원인인지 증명 -> 재현해 내는 것은 어려움
- 실제로는 확실한 요소만 검출
	- 시스템의 복잡성 : 단일 모듈은 문제가 없으나, 복합적으로 동작할때 문제가 발생 (특정한 경로, 상태 등에 의존적이기 때문)
	- 운영 중인 프로덕션 시스템에서 문제를 재현하지 말라.
		- 재현의 어려움
		- 불필요한 다운타임 발생
		- 프로덕션과 동일 환경 구성 


## 조금 더 수월하게 장애를 조치하기
- 화이트 박스 지표와 구조화된 로그를 모두 활용 각 컴포넌트를 관찰하는 방법을 마련
- 시스템을 디자인 할때 컴포넌트 간 이해가 쉽고 관찰이 가능한 인터페이스 마련

- 일관된 방법으로 시스템에 관한 정보를 얻을 수 있다면 각각 컴포넌트 들의 로그 항목과 장애 항목을 대조하지 않아도 되어 빠른 분석과 복구가 가능해 짐

## 결론
> 장애조치를 경험이나 운에 의존하지 않고,
> 시스템적으로 해결하는 접근법을 도입하면 시스템의 복구 시간이 단축된다.

