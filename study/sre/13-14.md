# 12. 사례연구

https://landing.google.com/sre/book/chapters/effective-troubleshooting.html

- 구글앱엔진 장애 사례
- 개발자 문서 작성용 CMS를 운영하는 고객문의
	- 지연응답
	- CPU 사용률 증가 
	- 앱 트래픽 서비스 프로세서  숫자 증가 (예측한 것 이상의 스케일아웃)


![](https://landing.google.com/sre/book/images/srle-1203.jpg)
초당 요청 수 

![](https://landing.google.com/sre/book/images/srle-1204.jpg)
요청 지연 발생 그래프

![](https://landing.google.com/sre/book/images/srle-1205.jpg)
cpu 사용률 변화

![](https://landing.google.com/sre/book/images/srle-1206.jpg)
인스턴스 수 변화

- 트래픽이 증가 했거나 시스템 설정이 변경된 경우 보통 발생되는 상황.
  - 리소스 사용률에 변화가 있었어도 원래대로 돌아갔어야 하지만 원래상태로 돌아가지 않음
- 앱엔진 개발자들과 공유 -> 사용자가 앱엔진의 어떤 기능을 사용하였는지 조사
	- merge_join 데이터 저장소 API  사용률 증가와 연관 됨을 발견 (의심)
		- `이 API는 데이터 저장소에서 데이터를 읽을때 사용하는 차선최적화 인덱스가 지적을 받은 경우가 종종 있었다`
	- 앱이 사용하는 객체의 속성에 복합 인덱스 추가 하여 요청 처리 속도를 개선
-  정확하게 문제가 되는 인덱스 속성을 찾기위해,
- 대퍼(Dapper)를 사용
	- 개별 http요청이 처리되는 각 단계를 추척, 각 서버들의 rpc 요청을 분석
	- 이미지 같은 정적 콘텐츠 요청은 DB가 서비스하지 않는 데이터 보다 매우 느리게 동작함을 알게됨.
	- 실제로는 merge_join이 문제가 아니었으며, our suboptimal-indexing theory was fatally flawed.
	- 고객의 앱이 뭔가를 수행하는데 250ms를 소비 -> 알수없음.
- cpu높은 인스턴스로 스케일업하고 느긋한 분석시작
- 앱의 인스턴스 초기화 시 -> 설정을 DB에서 가져와서 메모리에 저장하여 해당 객체를 참조
- 이경우 처리하는 요청을 처리하는 시간은 설정 데이터의 양과 비례해 늘어남
	- 매 요청시, 사용자가 지정된 경로에 대한  무조건적권한(whitelist)이 있는지 확인하는 메서드 의심
	- `제한 없는 객체들을 인스턴스의 메모리에 저장함으로써 데이터 저장소와 멤캐시 서비스 모두에게서 권한을 검색하기 위한 캐시 계층처럼 사용되고 있었다.`

- 원인은?
	- 권한 제어 시스템의 버그
	- 누군가 특정 경로에 대한 접근이 허용되면 해당 사용자가 whitelist 권한을 가진 객체들이 생성되어 데이터저장소에 저장
	- 앱 실행시 자동화된 보안스캐너가 앱의 취약점을 테스트하며,
	- **30분에 걸쳐 whitelist 객체를 수천개 만들어냄…..**
	- 앱은 매번 요청을 받을때마다 whitelist에서 해당 요청에 대한 권한 검사를 하여 느려짐.

구글의 사례의 경우 
	- 장애 상황을 분석할 수 있는 방안이 잘 마련되어있음

# 13. 긴급 대응 Emergency Response
- 긴급한 상황에 자연스럽게 대응하기 위해 철저한 준비와 정기적으로 실제 긴급 상황과 관련있는 실습수행이 필요

## 시스템에 문제가 생기면 어떻게 해야할까?
- **당황하지 말라**
- 부담스럽다면 다른사람에게 도움을 요청하면 된다. - 장애 대응 절차 참고

## 테스트로 인한 장애
- 구글의 경우 긴급 상황에 대한 사전적 테스트 접근법을 활용
- 시스템에 장애를 일으킨 후 이로 인해 시스템이 어떻게 실패하는지 관찰 후, 신뢰성 향상 방법 모색
	- 취약점 분석 및 의존성 발견시 문서화

### 세부 내용
- 분산 데이터 베이스에 테스트 데이터베이스를 추가하여 숨겨진 의존성 확인
- 특정 데이터 베이스의 접근을 차단 시키고 결과를 확인

### 대응
- 데이터베이스에 의존하는 여러 시스템 및 사용자가 핵심 시스템 접근 불가 확인
- 테스트를 중지 시켰으나 권한의 복구작업 실패.
- 기존의 테스트가 완료된 방법을 이용해 replication, failover 데이터베이스의 권한을 복구
- 개발자들에게 애플리케이션 레벨에세 DB 계층 라이브러리 수정 요청
-> 이 현상이 재발하지 않도록 정기적으로 테스트 수행 계획 수립

### 우리가 발견한 것들
#### 테스트를 진행하면서 잘한 부분
- 우리의 예상대로(?) 테스트의 영향이 걷잡을수 없이 커져 테스트를 중지
	- ~~정말 예상한 정도로만 영향이 있었는지는 의문~~
- 장애 첫 보고 이후로 1시간 안에 모든 권한을 복구

#### 깨달은 사실
- 철저한 리뷰를 거쳐 범위를 규정했다고 생각했으나, 시스템들 간의 상호작용 이해 부족 확인
- 테스트 환경에서의 롤백 절차를 테스트한적이 없어, 롤백 절차에 대해 결함 발생 확인
- -> 대량의 테스트를 진행하기에 앞서 **롤백 절차에 대한 완벽한 테스트**가 필요

## 변경으로 인한 장애
- 구글은 어마어마한 수의 설정 (+ 엄청 복잡함)을 관리하며, 지속적인 변경이 발생
- 구글 인프라스트럭쳐의 규모와 복잡도 때문에 모든 의존성, 상호 작용을 일일이 확인하기는 불가능

### 세부 내용
- 서비스의 악의적 사용 방지를 위한 인프라 설정 적용
- 외부로 노출되는 서비스들과 동작하는 인프라의 설정 변경
- 시스템 시작 후 크래시 루프 발생 (반복적인 시스템 중단 발생)
- 해당 인프라와 관련된 내부 서비스들도 사용불가

### 대응
- 특정 사이트 다운에 대한 모니터링 알람이 쏟아짐
- 비상대기엔지니어 들이 사내 네트워크 장애에 대한 알람 수신
- 프로덕션 환경에 대한 백업용 접근이 가능한 공간으로 모여 장애 처리
- 5분 후 첫 설정을 롤백 - 설정을 변경한 엔지니어에 의해.
- 10분 장애 선언 후 장애 상황 전파
	- 설정을 변경한 엔지니어가 장애의 원인 및 롤백한 상황을 알림
- 그와는 별개로 일부 서비스들은 그와 무관한 버그 발생
- 변경된 설정에 영향을 받아 생성된 다른 설정 때문에 장애 상황이 지속됨

### 우리가 발견한 것들
#### 장애를 처리하면서 잘한 부분
- 이 장애가 구글의 많은 내부 시스템의 장시간의 장애로 이어진 요인 -> 단시간 엄청난 중복알람
	-> 대화 및 장애대응 대화 채널에 방해를 줌
- 신속하고 명확한 장애 관리 절차 진행
- 왜 고가용의 오버헤드가 낮은 백업시스템이 필요한지 알게 해준 사례
- 설정 변경 롤백 도구 등이 원활이 동작, 엔지니어는 도구를 잘 숙지하고 정기적 테스트를 진행

#### 깨달은 사실
- 새 기능 배포에 앞서 카나리 테스트를 수행했으나 설정과 기능이 조합된 부분에 테스트가 제대로 이루어 지지 않음
- 설정 변경에 대한 부분에 있어 위험도는 낮게 측정하여 느슨하게 진행됨.
- 해당 장애로 인해, 카나리 테스트와 자동화 개선에 더 높은 우선순위를 두게 됨


## 절차에 의한 장애
- 자동화 시스템 구축에 많은 시간과 노력을 들임
- 장비의 교체가 쉬운 환경
	- 하지만 잘못 동작한다면 자동화의 효율이 무서운 결과를 초래

### 세부내용
- 자동화 테스트를 진행중 사용이 종료된 두개의 시스템 종료 요청이 연속적으로 전달됨
- 두번째 시스템 종료 요청 
	- -> 자동화 시스템의 버그로 인해 전체 인프라에서 동일한 설정으로 설치된 모든 서버에 디스크 삭제 요청이 큐에 기록. 
- **모든 서버의 하드 드라이브가 삭제될 위기**💣

### 대응
- 비상대기 엔지니어에게 첫번째 서버가 오프라인 모드 알람이 전달
- 해당 머신의 디스크 삭제 큐의 요청을 파악
- 비상대기 엔지니어는 해당 지역의 트래픽을 여력이 있는 다른 지역으로 우회 시킴
	- 해당 지역의 머신들이 모두 제거되어 요청에 응답이 불가능..
- 전세계 동일 종류 서버로 부터 알람이 발생
	- 추가 피해가 발생하지 않도록 모든 팀의 자동화 시스템을 비활성화 시킴
	- 프로덕션 유지보수를 일시적으로 중단
	- 트래픽은 다른지역으로 분산하여 요청을 정상적으로 처리
- 복구 절차 진행
	- 네트워크 링크에서 과부하가 발생하여 병목 구간을 확인한 네트워크 엔지니어가 트래픽을 줄이는 조치
	- 부하가 발생한 지역 중 한곳의 서버설정을 선택하여 서버를 증설
	- 3시간 이후 증설이 완료되고 요청을 처리하게 됨
- 3일쯤 지나서 대부분의 서버가 복구..

### 우리가 발견한 것들
#### 장애를 처리하면서 잘한 부분
- 대량 서버의 앞에 놓인 리버스 프록시는 소량 서버를 처리하는 리버스 프록시와 관리 방법이 매우 달라 영향을 크게 받지 않음 (??)
- 비상대기 엔지니어는 트래픽을 소량의 서버군으로부터 대량의 서버군으로 빠르게 옮길수 있었음
	- 대량의 서버군은 극한의 부하에 견딜 수 있음
- 그러나 네트워크 부하가 몰리면서 우회방안 모색
	- 비상 대기 엔지니어는 부하가 높은 네트워크에 집중
	- 여력이 있는 다른지역으로 트래픽을 우회
- 소량 서버군에 대한 시스템 종료 과정은 잘 동작
- 자동화된 시스템 종료처리가 매우 빠르게 일어났으나 
	- 모니터링 시스템이 잘 동작하여 신속하게 포착할수 있었음

#### 깨달은 부분
- 시스템 종료 자동화 서버가 전달된 명령의 유효성 판단을 제대로 하지 못해 발생
- 첫번째 서버 복구 후, 시스템 종료 자동화 서버는 빈응답을 받았으나, 이를 걸러내지 못하고 DB에 빈값을 넣어 모든 서버를 삭제하는 요청을 하게 됨..
	- (빈 값이 전부를 의미하는 경우의 문제..)
- 머신 재설치 인프라스트럭처는 수천대에 대한 작업을 처리하지 못함
	- 워커당 최대 2 개의 설치작업을 실행하는 제한..
	- Qos설정 문제 및 타입아웃 설정..
	- 디스크 삭제 중인 시스템에 설치 진행…..
	- 해당 인프라스트럭처를 재설정 할 수 있는 인력에 도움 요청
	- 

## 모든 문제가 해결 되었다
- 단순히 동작하지 않는 상황뿐 아니라 예상치 못한 방법으로 문제가 발생을 경험
- 해결책이 생각나지 않는다면 도움을 요청해야 함

> 가장 중요한 사람은 **사건이 발생하게 된 행위를 한사람** 그 사람을 잘 활용해야 한다.

## 지난 일로부터 배우기. 그리고 반복하지 않기
### 장애에 대한 기록을 남기자
- 무언가를 배우는데 문서로 남기는것보다 나은방법은 없다.
- 기록은 다른 이의 실수를 바탕으로 학습을 하는것
	- 광범위하고 솔직하게 작성
	- 전술적인 면 뿐아니라 전략적으로 재발을 방지하기 위한 실행요소를 찾아야 함
	- 포스트모텀을 진행하여 사내 모든 사람이 장애에 대한 학습 내용을 공유

### 커다란, 어쩌면 불가능 할지도 모를것에 대한 질문을 던지자
- 끊임없이 장애 상황에 대한 질문을 하자
	- 전기가 끊긴다면? 네트워크 장비가 물에 잠긴다면? 
	- 아마존이 망한다면? 데이터센터가 불이난다면?
	- 누가 진행 상황을 체크할까?
	- 어떤 계획이 있는가? 나는 무엇을 하나? 내 시스템은 어떻게 동작할 수 있을까?

## 결론
- 장애를 조치하는 사람은 침착해야 한다
- 필요한 경우 다른 사람에게 도움을 요청할 수 있어야한다
- 시스템이 동일 종류에 장애에 잘 대처할수 이도록 개선해야 한다
- 실제 장애가 발생하기전에 취약점을 발견할 수 있도록 사전 테스트에 주의를 기울여야한다.


# 14. 장애 관리하기 Managing Incidents
효과적인 장애 관리는 장애로 인한 피해를 최소화하고 최대한 빨리 평소의 비즈니스 운영을 복구하는 것
체계적이지 못한 장애 관리의 사례와 올바른 관리 가이드라인으로 동일 장애상황 대응 차이점을 살펴보기

## 미흡한 장애 관리 (슬픈사례)
- 오후2시.여러군데의 데이터 센터가 미친듯한 트래픽으로 터져나가는 중
- 시스템은 모든 요청을 처리할 수 없는 상태
- 1. 로그확인 -> 최근 수정된 모듈에서 에러가 발생함 인지
- 2. 서버 개발자1,2,3 호출 (개발자의 지역시간은 새벽3시반)
- 3. 매니져의 압박 ‘사업적으로 중요한 서비스의 장애에 대해 왜 저에게 알리지 않으셨죠?’
- 4. 부사장의 압박 ‘언제 해결되나요? 대체 왜 이런일이 생긴거죠?’
- ㅂㄷㅂㄷ..
- 5. 또다른 개발자4(무관자) 호출 -> 프로덕션에 수정 적용
- 6. 서버들 사망… 😇

## 미흡한 장애 처리에 대한 자세한 분석
### 기술적인 문제에 대한 날카로운 집중
- 비상대기엔지니어 : 기술적인 작업이 너무 많았기 때문에 문제를 해소할 수 있는 큰 그림에 대해 생각할수가 없음.
### 소통의 부재
- 비상대기엔지니어 : 명확한 소통을 하기 어려운 상황. 동료가 어떤 일을 하고 있는지 아무도 모름. 디버깅이나 이슈의 해결을 도화줄 다른 엔지니어를 효과적으로 활용하지 못함.
- 고객과 비즈니스리더의 분노
### 프리랜서의 고용
- 개발자4는 자신이 최선이라고 생각하는 방향으로 시스템을 변경. 
- 장애조치의 책임을 맡고 있는 엔지니어와 협업을 하지 않음.

## 장애 관리 절차의 기본 요소들

### 책임에 대한 귀납적인 분리
- 장애 조치 참여자들이 자신의 역할을 이해하고 다른 이의 영역을 침범하지 않음. -> 개개인의 작업 자율성 보장
- 계획을 수립하는 리더는 미리 역할을 위임하고 정보를 보고 받음

- 장애조치 역할
	- 장애 제어 (장애 제어자)
		- 장애에 대한 높은 수준의 상태를 확인
		- 장애 조치 팀 구성. 우선순위 선정, 책임분배
		- 운영 업무 방해요소 제거
	- 운영 업무
		- 장애 제어자와 협력하여 실제 업무를 수행하며 장애에 대처
		- 장애 조치 중에는 **오직 운영팀만 시스템을 변경**해야한다.
	- 의사 소통
		- 장애 조치팀의 대외 창구역할 
		- 의사결정자들에게 상황을 보고하며, 장애조치 문서를 최신의 상태로 유지.
	- 계획
		- 운영팀을 도와 버그를 수집. 업무보조. 
		- 시스템 상태변경 추적 후 장애 조치 후 원래대로 복구 하는 등 장기적 대응
	
## 확실한 컨트롤 타워
- 장애 제어자와 명확한 소통이 필요. 
- 같은 공간에 함께 하는 것이 좋다
- 이메일이나 IRC로 상황을 지속적으로 공유

## 실시간 장애 조치 문서 
- 장애 제어자는 실시간으로 장애 조치 문서를 관리해야 함.
- 부록 C 참고
- 문서를 공유문서로 작성하여 동시 편집이 가능하게 한다.
- 포스트모텀 분석을 위해 계속 유지되어야하며, 메타 분석에도 활용할 수 있다.

## 명확하고 즉각적인 업무이관
- 장애 제어자의 일과가 끝나면 다른 담당자에게 이관
- 이관할 장애 제어자에 대해 공지
- 완전히 이관에 대한 인지가 끝날때까지 연락을 끊어서는 안된다.
- 장애를 조치하고 있는 사람들에게도 전달되어 누가 관리하고 있는지 명확히한다.

## 적절하게 관리한 장애 조치
- 오후2시. 여러군데의 데이터 센터가 미친듯한 트래픽으로 터져나가는 중
- 시스템은 모든 요청을 처리할 수 없는 상태
- 1. 장애 관리 절차를 따른다
- 2. 서버개발자 2 에게 장애 제어자 분담. 비상대기엔지니어는 기술업무수행
- 3. 이메일로 관련 수신자들에게 전달 및 장애 영향도 분석 
- - 세번째 데이터센터 장애 발생 - 
- 4. 장애제어자는 IRC 디버깅 채널에서 장애알림을 확인하고 이메일 스레드를 업데이트
- 5. 장애제어자는 엔지니어에게 개발자 지원 필요여부를 확인
- 6. 개발자 합류. 장애 조치 문서를 통해 현재 상태를 분석
- 장애제어자는 개발자들에게 업무수행에 대한 내용을 비상대기엔지니어에게 공유할 것을 당부
- 오후5시. 장애제어자의 퇴근시간. 장애 조치문서를 업데이트.
- 컨퍼런스 콜을 통한 상황 공유
- 오후6시. 다른 리 사무실의 동료들에게 역할을 이관.
- 다음날 출근 장애 조치를 완료하고 포스트모텀을 작성

## 언제 장애를 선언할 것인가
- 장애를 빨리 선언하고 최대한 간결한 해결책을 찾아 장애를 조치
- 다음중 하나라도 해당한다면 이를 장애로 선언한다
	- 문제를 해결하기 위해 다른 팀의 도움이 필요한가?
	- 문제가 사용자에게 영향을 미쳤는가?
	- 문제 발생 이후 한시간 동안 집중적으로 분석했으나 해결되지 않았는가?
	
- 장애 관리 숙련도는 지속적으로 활용하지 않으면 금방 쇠퇴
	- 여러 팀이 참여하는 운영 업무에 공통적으로 활용 가능
	- 이미 해결된 비상대기 이슈에 대해 역할을 실습
	
	
