# 11. 비상대기 Being On-Call
비상대기 - 서비스의 신뢰성과 가용성을 위해 반드시 수행해야할 임무

~~구글의 SRE들도 비상대기로부터 자유롭진 못하..~~

> 구글의 SRE 들이 수년에 걸쳐 개발한 비상 대기 업무 수행 방안의 기본적인 원리 소개

## 소개
- SRE 들이 서비스를 위한 비상 대기 업무를 수행
- 단순 운영이 아닌, 문제를 해결하기 위한 **엔지니어링적 접근법**을 강조
- 순수 운영 업무는 최대 50%의 시간으로 제한 
- 50%의 시간은 서비스 개선 및 자동화를 통해 업무 환경을 개선할 수 있도록 엔지니어링 프로젝트에 투입

## 비상 대기 엔지니어의 삶
> 비상 대기 엔지니어는 프로덕션 시스템의 보호자로, 
>
> 팀에 영향을 미치는 장애를 관리하고 프로덕션 환경의 변경을 추진 및 진단하는 등 할애된 운영 업무를 수행한다.

- 비상 대기 시에 엔지니어는 수 분 이내 운영작업을 수행해야함.
- 사전에 약속된 장애 대응 시간
	- 서비스중요도에 따라 5분 ~ 30분
	- 사용자 노출 서비스의 경우 분기별 가용성 99.99%를 확보 (다운타임 13분)
	- 분 단위 장애대응이 필요(SLO 낮은 경우에는 10분 단위로도 정의함)
- 이메일, SMS, 자동 전화, 모바일 앱등의 알림 시스템 운영
	- 우선순위가 낮은 알림 등은 업무 시간 중 처리
- 비상 대기조 - 보조 대기조 운영 (a primary and a secondary on-call rotation)
	- 비상 대기조가 알람 수신을 놓친 경우를 대비한 보조 대기조 운영 
	- 비상 대기조는 오로지 장애 알림처리만, 보조 대기조는 프로덕션 업무 수행

## 비상 대기 업무의 균형 맞추기
> 비상 대기 업무에 편성 되는 경우, 업무의 양과 품질에 제약이 있다.
>
> 비상 대기 업무의 양은 엔지니어가 비상 대기 업무에 할애한 시간의 백분율로 계산한다.
>
> 비상 대기 품질은 비상 대기 업무 기간동안 발생한 장애의 수로 계산 될수 있다.

### 업무 양의 균형

- 50% 엔지니어링 프로젝트, 25% 비상 대기, 25% 운영 업무
- 25% 비상 대기 -> 24/7 대기하는 비상 대기조 운영이 가능 (계산법 ‘-‘??)
	- 단일사이트 : 두사람이 주/보조 로 대기. 8명. 4주에 1주는 비상대기.
	- 다중사이트 : 6명
		- 야간 교대를 하지 않는다.
		- 비상 대기 업무에 참여하는 엔지니어의 수를 제한 -> 엔지니어들의 시스템에 대한 관심 지속
		- 의사소통과 협업에 더 많은 비용을 소모

### 품질의 균형

- 비상 대기 시 장애의 주요 원인 분석과 개선, 포스트 모텀 작성, 버그 수정과 같은 후속 작업등으로 평균 6시간 소요. 
- 비상대기 교대는 매 12시간 마다 이루어지므로 이 부석에 따르면 하루에 처리할수 있는 최대 장애는 2개.
- 어떤 컴포넌트나 이슈가 매일 호출을 발송한다면 (평균 장애 수/일 수 > 1) 특정 시점에 다른 어딘가에 장애가 발생하여 평소보다 많은 장애가 발생.

> 한계값이 분기당 한번 정도와 같이 일시적으로 초과 된다면 이를 보정하기 위한 측정을 도입하여 `운영 부하`를 지속 가능한 상태로 유지해야함.

### 보상
- 구글의 경우 대체휴무나 전체 연봉에 대한 일정 비율의 현금보상을 제공
- 보상의 상한이 존재하면 특정 개인이 비상 대기 업무에 투입되는 시간을 제어

> 보상 정책을 통해 비상 대기 업무에 투입되는 부분에 대한 동기를 부여
>
> 비상 대기 업무의 균형적인 분산과 피로 혹은 프로젝트 업무 수행을 위한 시간 부족등 초과 비상대기 업무로 인한 잠재적 부작용 해소

## 안전에 대해 고려하기

> 사용자가 직접 사용하며, 수익과 관련된 시스템이나 이런 시스템들을 지속적으로 운영하기 위해 필요한 인프라에 대한 관리 책임을 진다는 것을 의미

- 복잡한 시스템의 장애를 처리할때는 합리적, 집중적, 그리고 계획적이며 경험에 기반한 선택이 효과적
- 엔지니어들이 자신을 제어할수 있도록 비상대기와 관련된 스트레스를 줄여주는 것이 중요
    - 서비스의 중요도 및 영향도와 잠재적 장애의 결과는 비상 대기를 수행중인 엔지니어에게 압박
    - 코르티솔, 코르티코트로핀 같은 스트레스 호르몬이 인지적 행위에 부정적인 영향을 미쳐 부적절한 의사결정
- 스트레스 호르몬의 영향을 받지 말고 무계획적으로 대응해 보자.
    - -> 시행착오를 통한 자기 학습 (~~몸빵…~~) -> 
    - 동일 장애 알림 발생 시, 이전 대응 경험에 의해 같은 원인이라 치부하여 쉽게 생각하게 됨.
        - 직감(경험)에 기반한 신속한 대응이 미덕이나, 명확한 데이터에 근거한 지원으로 이어지지 않을 수 있음. -> 시간낭비 가능성

> 신속한 대응 또한 습관에 의해 좌우
>
> 자신의 추측을 심도있게 확인하는 동시에 좀 더 합리적인 의사 결정을 위해 
>
> 충분한 데이터를 활용할 수 있는 시점에 적절한 속도로 차근차근 단계를 밟아가는 완벽한 균형이 필요

<비상대기 업무에 대한 부담을 줄이는 자원들>
- 분명한 장애 전파 경로 Clear escalation paths
	- 시스템 개발 팀이 비상대기에 투입되며, 필요하다면 다른팀에 장애를 전파 할 수 있다.
- 잘 정의된 장애 관리 프로세스 Well-defined incident-management procedures
	- 장애의 수준이 복잡하고 장애해결에 소요시간이 불분명한 경우 장애 관리 절차를 따르는 것이 좋다.
	- 구글에서는 장애 절차를 웹 기반 도구를 통해 역할을 넘겨주거나 상태변화를 녹화 공유 장애 관리가 자동화 되어있다.
- 비난 없는 포스트모텀 문화 A blameless postmortem culture
	- 언제 무엇이 잘못 되고, 어떤 부분이 제대로 동작했는지 판단해서 향후에 동일한 에러가 반복하지 않도록 기록해야한다.
	- 사람이 아닌 사건에 집중하여 기술
	- 프로덕션환경의 장애를 시스템적으로 분석
	- 문제를 정의하여 자동화 할 수 있는 부분을 선별

## 부적절한 운영 부하에서 벗어나기
### 운영부하

이상적으로는 운영 업무 부담의 증가에 대한 증상을 측정할 수 있어서 그 목표치가 정량화 될 수 있어야 한다. 

(티켓Task 5개 미만, 비상 대기 동안 호출 알림 2회 미만 )

1. 모니터링 오설정

호출 알림은 서비스의 SLO를 위협하는 증상이 발생하는 경우 발송.

우선순위가 낮은 알림이 빈번한 경우, 생산성 저하 및 알림을 간과하게 되어 심각한 알림을 놓칠 가능성

우선순위 낮은 중복알림 비활성화

2. SRE 외부의 문제

애플리케이션 개발자의 작업과정에 시스템의 신뢰성에 문제 발생

해당 개발자와의 협업을 통해 시스템 향상을 위한 공통 목표 설정

…극한의 상황시 SRE가 개발팀에게 한시적으로 비상 대기에 집중해줄 것을 요청
	- 여러 부분에 영향을 미치는 복잡한 변경이나 아키텍처적인 변경을 적용 시, 시스템 안정화를 위해 요청

## 뜻밖의 적: 운영 업무 부족
- 시스템이 이상할만큼 문제가 없어서 비상대기 투입 빈도가 낮아짐 
	- -> 프로덕션 환경에 대한 자신감 하락 (또는 부족)
	- -> SRE의 지식과 프로덕션 환경의 차이를 장애가 발생하기전엔 알 수 없음
- 모든 SRE를 적절하게 비상대기 투입, 프로덕션 환경에 노출 -> 장애를 접하며, 서비스에 대한 문제 해결 능력과 지식을 갈고 닦을 기회를 얻을 수 있다..
- 구글은 연간 장애 복구 대회 개최(Disaster Recovery Training, DiRT) 



# 12. 효과적인 장애 조치 Effective Troubleshooting

> 단순히 시스템의 동작을 이해한다고 해서 전문가가 될 수 있는 것은 아니다.
> 
> 전문성이라는 것은 시스템이 동작하지 않는 이유에 대해 연구하는 과정에서 얻어지는 것이다.
> 
> -브라이언 레드맨

- 장애조치를 어떻게 해야하는지 설명하는 것은 자전거 타는 방법을 설명하는 것처럼 어렵다.

## 이론
- 가설귀납법적 방법
	- 시스템 관찰에 대한 결과와 시스템의 행동에 대한 이해를 기반으로 장애 원인에 대한 가설을 세우고 가설을 시험

![](https://landing.google.com/sre/book/images/srle-1201.jpg)

1. 시스템 측정 데이터와 로그를 통해 현재 상태 파악
2. 시스템 구현, 동작 방식 과 장애 복구 지식을 결합하여 원인을 규명
3. 가설을 테스트 (시스템의 상태를 이론과 비교하여 정황 분석, 통제된 방법으로 시스템을 수정 및 관찰)
4. 장애 재발을 방지하기 위해 장애 조치후 포스트모텀 문서를 작성

### 일반적인 문제
- 시스템에대한 깊은 이해가 부족하여 비효율적인 장애 조치 절차로 흘러감
	- 장애 등급 선정, 분석 및 진단 단계에 영향
- 관련이 없는 증상을 들여다보거나 시스템의 지표의 의미를 잘못이해하는 경우
- 시스템의 변경이나 입력 값 혹은 환경에 대한 잘못된 이해는 안전하고 효과적인 가설의 검증에 방해
- 장애 원인에 대한 가능성이 희박한 가설을 세우거나 과거 발생한 문제의 원인과 결부시켜 발생한 문제가 다시 발생할 것이라 결부해버리는 행위
- 우연히 발생했거나 동일한 원인에 의해 발생한 관련 현상들을 계속해서 쫓아다니는 행위

## 실전에 들어가보자
## 문제 보고
- 실제로 기대한 동작은 무엇인지, 그리고 현재 어떻게 동작하고 있는지, 문제가 되는 동작을 어떻게 재현할수 있는지 설명
- 버그 추적 시스템과 같이 검색이 가능한 위치에 저장
- 문제를 특정인에게 직접 보고하는 것은 지양

## 문제의 우선순위 판단
- 문제의 영향도에 따라 적절히 대처 방법을 선택
- `우선 시스템이 가능한 정삭적으로 동작하게 만드는 것이 목표`
	-  다른 클러스터에 트래픽을 전환, 연쇄적인 장애를 피하기 위해 트래픽 감소, 부하를 줄이기 위해 서브시스템 비활성화

	- 데이터 복구가 불가능할정도로 손상을 입었다면 손실방지를 위해 시스템 중단
- 파일럿은 긴급상황에서 자신의 최우선 과제는 비행기를 계속 비행하게하는것. 비행기와 그외 모든것들을 안전하게 착륙

## 문제를 관찰하기
- 이상적인 경우 모니터링시스템이 모든 지표를 기록
- 로그를 통해 각 동작에 대한 정보와 시스템의 상태를 살펴보며 특정 시점에 작업한 프로세스에 대한 이해
- 대퍼를 이용하여 전체 스택에 걸쳐 요청을 추적하는것은 분산시스템이 어떻게 동작하는지 이해할수 있는 방법
- 상태를 외부에 노출 하는 방법 
	- 최근 발수신 RPC 를 보여주는 종단점이 있음. 아키텍처 다이어그램이 없어도 서버 사이의 통신 수행을 이해. 
	- 설정을 보여주거나 데이터를 확인할 수 있는 종담점을 제공

## 진단
- 시스템 진단 기법을 사용하면 시스템에 전반적인 이해가 부족해도 문제 원인 파악을 할수 있다.

### 단순화하기와 범위를 좁히기
- 시스템 내의 컴포넌트 들은 잘 정의된 인터페이스와 더불어 입력 데이터를 출력데이터로 변환할수 있어야 함
- 컴포넌트 사이의 데이터 흐름을 확인하여 올바른 동작 여부를 확인
- 테스트 데이터를 통해 의도적 에러를 유발하기도 함
- 시스템을 각 계층별로 분할 정복기법은 범용적으로 해결책을 찾기위한 유용한 수단
	- 소규모의경우, 다계층의 컴포넌트 스택을 순차적으로 테스트를 진행하며 각 컴포넌트가 제대로 동작하는지 확인
	- 시스템이 거대한 경우, 시스템을 반으로 나누어 컴포넌트 사이의 통신 경로를 확인. 나머지 반쪽에 대해서 문제확인 수행. 

### 무엇이, 어디서, 왜를 고민하기
- 오동작 시스템은 우리가 원하는 동작 이외의 다른 동작을 계속적 실행
- 시스템의 동작을 확인 -> 왜 수행하는지 -> 자원을 어디서 사용하는지 -> 결과가 어디로 전달

### 가장 마지막으로 수정된 부분에 주목

> 시스템의 항상성. 올바르게 동작하는 컴퓨터 시스템은 설정의 변경이나 서비스 부하의 종류가 바뀌는 외부요인이 발생하기 전까지는 계속해서 동작 
> 
> -> 그래서 무엇이 잘못되고 있는지 파악하기 좋은 시작은 **최근 변경 지점**


**<새로운 버전 배포 시작 시점에서 완료까지 에러율>**
![](https://landing.google.com/sre/book/images/srle-1202.jpg)

### 서비스에 특화된 진단
- 특정 서비스를 분석하는데 도움이 되는 도구와 시스템을 개발하는 것이 좋다.
- 구글의 SRE들은 대부분의 시간을 이런 도구들을 개발하는데 사용
- 대부분의 서비스와 팀 간 공통성을 확인함으로써 중복된 노력을 제거하기위해 시스템 특화적으로 개발

## 테스트와 조치
- 가능한 원인을 파악 후 테스트를 통한 근본 원인 색출
- 코드의 흐름을 단계별로 모방해보며 문제점 색출

<예시>
- 애플리케이션 서버와 데이베이스 서버간 접속 장애 발생 시 가설검증
	- 동일인증정보로 DB 접근(DB 연결 거부), DB ping(네트워크 문제)

### 테스트 구상의 주의할 점

- 테스트는 상호 배타적이어서 가설의 어느 한 집합을 검증함으로써 다른 가설의 가능성이 없음

- 가장 명확한 것을 최우선으로 고려
	- 가능성이 큰 테스트부터 수행
	- 테스트로 인한 시스템의 장애 발생 가능성 위험을 고려

- 혼란 요소로 인한 특정 실험이 잘못된 결과를 도출할 가능성 고려
	- 방화벽 정책이 특정 IP 요청에만 응답하도록 적용 -> 서버내에선 문제가 없지만 외부 사용자에게는 실패

- 적극적인 테스트가 나중에 실행할 테스트 결과에 부작용을 초래
	- 테스트를 위한 프로세스가 cpu 점유를 과하게 설정하면 경쟁상태 발생 가능성 증가
	- 과한 양의 로그를 기록하도록 설정하여 지연 응답 문제 발생
	- 이런 경우 문제해결에 대한 정확한 판단이 어려움

- 일부 설득력이 떨어지는 테스트
	- 정기적이면서 반복가능한 형태로 경쟁상태나 데드락 상황을 만들기는 어려움
	- 문제의 원인으로 규명하기에 불확실한 증거임에도 만족할수 밖에 없음..

- 테스트 수행 여부 및 결과를 기록
	- 복잡하고 긴 시간을 소요하는 테스트는 **정확하게 어떤 현상이 발생** 하였는지 기록
	- 같은 과정을 반복하는 상황 제거
	- 시스템 원복에 도움이 됨
 

# 부정적인 결과의 마법
> `부정적인`결과는 기대한 효과가 나타나지 않은 경우에 대한 **경험의 산물**
> 계획대로 되지 않은 모든 실험

### 부정적인 결과는 무시해서도 안되고 평가절하해서도 안된다.

- 잘못에 대한 인식 그 자체에 가치

### 부정적인 결과로 끝난 실험 역시 결론이다.
- 결과를 통해 프로덕션 환경이나 디자인 공간 혹은 기존 시스템 성능 한계등을 확인할 수 있음
- 추후 그 실험 결과 문서를 통해 부정적인 결과를 초래할 시스템 디자인에 대한 평가를 빠르게 할 수 있음
- 마이크로벤치마크와 문서화된 안티패턴들, 그리고 프로젝트 포스트모텀은 이 범주에 해당
- 싫험을 계획할 때는 도출 가능한 부정적인 결과에 고려해야 함 (의미가 확실한 부정적 결과는 도움이 됨)

### 도구와 방법은 실험의 결과와는 무관하며 향후의 작업에 대한 단서가된다.
- 벤치마킹 도구들과 부하테스트 도구를 통해 기대에 미치지 못하는 실험을 긍정적인 방향으로 활용
- 아파치 벤치 같은 웹서버 부하테스트 도구들을 사용하며, 장기적으로 어렵고 세부적인 작업 내역을 바탕으로 많은 장점을 얻음
-> 누적된 툴/스크립트 사용 경험이 다음 애플리케이션 작업에 도움이 됨

### 부정적인 결과를 공표하는 것은 업계의 데이터 주도 성향을 증진시킨다

- 부정적인 결과와 통계적으로 그다지 의미가 없는 데이터에 대한 자세한 기록은 
- 우리가 가진 지표에 대한 편견을 해소하고 다른 사람들로 하여금 측정된 데이터에 근거해 불확실성을 이해하는 좋은 예시
- 결과를 공유하여 다른 이들도 같은 방법을 취하게끔 유도할 수 있으며, 업계의 모든 이들이 이를 일괄적으로 학습하게 할 수 있음
- 프로덕션 환경의 안정성에 커다란 긍정적 효과

### 자신의 결과를 공표하자
- 자신의 실험 결과를 공개하여 유사한 실험에 관심이 있는 사람에게 정보를 제공
- 자신이 배제한 디자인, 알고리즘, 팀의 업무 흐름에 대해 공유
- 부정적인 결과는 신중하게 위험을 받아들이는 과정의 일부
- **실패를 언급하지 않은 모든 디자인 문서, 성능 리뷰, 에세이 들은 의심하라.**
	- 많은 생략, 면밀히 검토하지 않았을 가능성.

## 처방
- 원인이 실제로 문제의 원인인지 증명 -> 재현해 내는 것은 어려움
- 실제로는 확실한 요소만 검출
	- 시스템의 복잡성 : 단일 모듈은 문제가 없으나, 복합적으로 동작할때 문제가 발생 (특정한 경로, 상태 등에 의존적이기 때문)
	- 운영 중인 프로덕션 시스템에서 문제를 재현하지 말라.
		- 재현의 어려움
		- 불필요한 다운타임 발생
		- 프로덕션과 동일 환경 구성 


## 조금 더 수월하게 장애를 조치하기
- 화이트 박스 지표와 구조화된 로그를 모두 활용 각 컴포넌트를 관찰하는 방법을 마련
- 시스템을 디자인 할때 컴포넌트 간 이해가 쉽고 관찰이 가능한 인터페이스 마련

- 일관된 방법으로 시스템에 관한 정보를 얻을 수 있다면 각각 컴포넌트 들의 로그 항목과 장애 항목을 대조하지 않아도 되어 빠른 분석과 복구가 가능해 짐

## 결론
> 장애조치를 경험이나 운에 의존하지 않고,
> 시스템적으로 해결하는 접근법을 도입하면 시스템의 복구 시간이 단축된다.



# 13. 긴급 대응 Emergency Response
- 긴급한 상황에 자연스럽게 대응하기 위해 철저한 준비와 정기적으로 실제 긴급 상황과 관련있는 실습수행이 필요

## 시스템에 문제가 생기면 어떻게 해야할까?
- **당황하지 말라**
- 부담스럽다면 다른사람에게 도움을 요청하면 된다. - 장애 대응 절차 참고

## 테스트로 인한 장애
- 구글의 경우 긴급 상황에 대한 사전적 테스트 접근법을 활용
- 시스템에 장애를 일으킨 후 이로 인해 시스템이 어떻게 실패하는지 관찰 후, 신뢰성 향상 방법 모색
	- 취약점 분석 및 의존성 발견시 문서화

### 세부 내용
- 분산 데이터 베이스에 테스트 데이터베이스를 추가하여 숨겨진 의존성 확인
- 특정 데이터 베이스의 접근을 차단 시키고 결과를 확인

### 대응
- 데이터베이스에 의존하는 여러 시스템 및 사용자가 핵심 시스템 접근 불가 확인
- 테스트를 중지 시켰으나 권한의 복구작업 실패.
- 기존의 테스트가 완료된 방법을 이용해 replication, failover 데이터베이스의 권한을 복구
- 개발자들에게 애플리케이션 레벨에세 DB 계층 라이브러리 수정 요청
-> 이 현상이 재발하지 않도록 정기적으로 테스트 수행 계획 수립

### 우리가 발견한 것들
#### 테스트를 진행하면서 잘한 부분
- 우리의 예상대로(?) 테스트의 영향이 걷잡을수 없이 커져 테스트를 중지
	- ~~정말 예상한 정도로만 영향이 있었는지는 의문~~
- 장애 첫 보고 이후로 1시간 안에 모든 권한을 복구

#### 깨달은 사실
- 철저한 리뷰를 거쳐 범위를 규정했다고 생각했으나, 시스템들 간의 상호작용 이해 부족 확인
- 테스트 환경에서의 롤백 절차를 테스트한적이 없어, 롤백 절차에 대해 결함 발생 확인
- -> 대량의 테스트를 진행하기에 앞서 **롤백 절차에 대한 완벽한 테스트**가 필요

## 변경으로 인한 장애
- 구글은 어마어마한 수의 설정 (+ 엄청 복잡함)을 관리하며, 지속적인 변경이 발생
- 구글 인프라스트럭쳐의 규모와 복잡도 때문에 모든 의존성, 상호 작용을 일일이 확인하기는 불가능

### 세부 내용
- 서비스의 악의적 사용 방지를 위한 인프라 설정 적용
- 외부로 노출되는 서비스들과 동작하는 인프라의 설정 변경
- 시스템 시작 후 크래시 루프 발생 (반복적인 시스템 중단 발생)
- 해당 인프라와 관련된 내부 서비스들도 사용불가

### 대응
- 특정 사이트 다운에 대한 모니터링 알람이 쏟아짐
- 비상대기엔지니어 들이 사내 네트워크 장애에 대한 알람 수신
- 프로덕션 환경에 대한 백업용 접근이 가능한 공간으로 모여 장애 처리
- 5분 후 첫 설정을 롤백 - 설정을 변경한 엔지니어에 의해.
- 10분 장애 선언 후 장애 상황 전파
	- 설정을 변경한 엔지니어가 장애의 원인 및 롤백한 상황을 알림
- 그와는 별개로 일부 서비스들은 그와 무관한 버그 발생
- 변경된 설정에 영향을 받아 생성된 다른 설정 때문에 장애 상황이 지속됨

### 우리가 발견한 것들
#### 장애를 처리하면서 잘한 부분
- 이 장애가 구글의 많은 내부 시스템의 장시간의 장애로 이어진 요인 -> 단시간 엄청난 중복알람
	-> 대화 및 장애대응 대화 채널에 방해를 줌
- 신속하고 명확한 장애 관리 절차 진행
- 왜 고가용의 오버헤드가 낮은 백업시스템이 필요한지 알게 해준 사례
- 설정 변경 롤백 도구 등이 원활이 동작, 엔지니는 도구를 잘 숙지하고 정기적 테스트를 진행

#### 깨달은 사실
- 새 기능 배포에 앞서 카나리 테스트를 수행했으나 설정과 기능이 조합된 부분에 테스트가 제대로 이루어 지지 않음
- 설정 변경에 대한 부분에 있어 위험도는 낮게 측정하여 느슨하게 진행됨.
- 해당 장애로 인해, 카나리 테스트와 자동화 개선에 더 높은 우선순위를 두게 됨


## 절차에 의한 장애
- 자동화 시스템 구축에 많은 시간과 노력을 들임
- 장비의 교체가 쉬운 환경
	- 하지만 잘못 동작한다면 자동화의 효율이 무서운 결과를 초래

### 세부내용
- 자동화 테스트를 진행중 사용이 종료된 두개의 시스템 종료 요청이 연속적으로 전달됨
- 두번째 시스템 종료 요청 
	- -> 자동화 시스템의 버그로 인해 전체 인프라에서 동일한 설정으로 설치된 모든 서버에 디스크 삭제 요청이 큐에 기록. 
- **모든 서버의 하드 드라이브가 삭제될 위기**💣

### 대응
- 비상대기 엔지니어에게 첫번째 서버가 오프라인 모드 알람이 전달
- 해당 머신의 디스크 삭제 큐의 요청을 파악
- 비상대기 엔지니어는 해당 지역의 트래픽을 여력이 있는 다른 지역으로 우회 시킴
	- 해당 지역의 머신들이 모두 제거되어 요청에 응답이 불가능..
- 전세계 동일 종류 서버로 부터 알람이 발생
	- 추가 피해가 발생하지 않도록 모든 팀의 자동화 시스템을 비활성화 시킴
	- 프로덕션 유지보수를 일시적으로 중단
	- 트래픽은 다른지역으로 분산하여 요청을 정상적으로 처리
- 복구 절차 진행
	- 네트워크 링크에서 과부하가 발생하여 병목 구간을 확인한 네트워크 엔지니어가 트래픽을 줄이는 조치
	- 부하가 발생한 지역 중 한곳의 서버설정을 선택하여 서버를 증설
	- 3시간 이후 증설이 완료되고 요청을 처리하게 됨
- 3일쯤 지나서 대부분의 서버가 복구..

### 우리가 발견한 것들
#### 장애를 처리하면서 잘한 부분
- 대량 서버의 앞에 놓인 리버스 프록시는 소량 서버를 처리하는 리버스 프록시와 관리 방법이 매우 달라 영향을 크게 받지 않음 (??)
- 비상대기 엔지니어는 트래픽을 소량의 서버군으로부터 대량의 서버군으로 빠르게 옮길수 있었음
	- 대량의 서버군은 극한의 부하에 견딜 수 있음
- 그러나 네트워크 부하가 몰리면서 우회방안 모색
	- 비상 대기 엔지니어는 부하가 높은 네트워크에 집중
	- 여력이 있는 다른지역으로 트래픽을 우회
- 소량 서버군에 대한 시스템 종료 과정은 잘 동작
- 자동화된 시스템 종료처리가 매우 빠르게 일어났으나 
	- 모니터링 시스템이 잘 동작하여 신속하게 포착할수 있었음

#### 깨달은 부분
- 시스템 종료 자동화 서버가 전달된 명령의 유효성 판단을 제대로 하지 못해 발생
- 첫번째 서버 복구 후, 시스템 종료 자동화 서버는 빈응답을 받았으나, 이를 걸러내지 못하고 DB에 빈값을 넣어 모든 서버를 삭제하는 요청을 하게 됨..
	- (빈 값이 전부를 의미하는 경우의 문제..)
- 머신 재설치 인프라스트럭처는 수천대에 대한 작업을 처리하지 못함
	- 워커당 최대 2 개의 설치작업을 실행하는 제한..
	- Qos설정 문제 및 타입아웃 설정..
	- 디스크 삭제 중인 시스템에 설치 진행…..
	- 해당 인프라스트럭처를 재설정 할 수 있는 인력에 도움 요청
	- 

## 모든 문제가 해결 되었다
- 단순히 동작하지 않는 상황뿐 아니라 예상치 못한 방법으로 문제가 발생을 경험
- 해결책이 생각나지 않는다면 도움을 요청해야 함

> 가장 중요한 사람은 **사건이 발생하게 된 행위를 한사람** 그 사람을 잘 활용해야 한다.

## 지난 일로부터 배우기. 그리고 반복하지 않기
### 장애에 대한 기록을 남기자
- 무언가를 배우는데 문서로 남기는것보다 나은방법은 없다.
- 기록은 다른 이의 실수를 바탕으로 학습을 하는것
	- 광범위하고 솔직하게 작성
	- 전술적인 면 뿐아니라 전략적으로 재발을 방지하기 위한 실행요소를 찾아야 함
	- 포스트모텀을 진행하여 사내 모든 사람이 장애에 대한 학습 내용을 공유

### 커다란, 어쩌면 불가능 할지도 모를것에 대한 질문을 던지자
- 끊임없이 장애 상황에 대한 질문을 하자
	- 전기가 끊긴다면? 네트워크 장비가 물에 잠긴다면? 
	- 아마존이 망한다면? 데이터센터가 불이난다면?
	- 누가 진행 상황을 체크할까?
	- 어떤 계획이 있는가? 나는 무엇을 하나? 내 시스템은 어떻게 동작할 수 있을까?

## 결론
- 장애를 조치하는 사람은 침착해야 한다
- 필요한 경우 다른 사람에게 도움을 요청할 수 있어야한다
- 시스템이 동일 종류에 장애에 잘 대처할수 이도록 개선해야 한다
- 실제 장애가 발생하기전에 취약점을 발견할 수 있도록 사전 테스트에 주의를 기울여야한다.


# 14. 장애 관리하기 Managing Incidents
효과적인 장애 관리는 장애로 인한 피해를 최소화하고 최대한 빨리 평소의 비즈니스 운영을 복구하는 것
체계적이지 못한 장애 관리의 사례와 올바른 관리 가이드라인으로 동일 장애상황 대응 차이점을 살펴보기

## 미흡한 장애 관리 (슬픈사례)
- 오후2시.여러군데의 데이터 센터가 미친듯한 트래픽으로 터져나가는 중
- 시스템은 모든 요청을 처리할 수 없는 상태
- 1. 로그확인 -> 최근 수정된 모듈에서 에러가 발생함 인지
- 2. 서버 개발자1,2,3 호출 (개발자의 지역시간은 새벽3시반)
- 3. 매니져의 압박 ‘사업적으로 중요한 서비스의 장애에 대해 왜 저에게 알리지 않으셨죠?’
- 4. 부사장의 압박 ‘언제 해결되나요? 대체 왜 이런일이 생긴거죠?’
- ㅂㄷㅂㄷ..
- 5. 또다른 개발자4(무관자) 호출 -> 프로덕션에 수정 적용
- 6. 서버들 사망… 😇

## 미흡한 장애 처리에 대한 자세한 분석
### 기술적인 문제에 대한 날카로운 집중
- 비상대기엔지니어 : 기술적인 작업이 너무 많았기 때문에 문제를 해소할 수 있는 큰 그림에 대해 생각할수가 없음.
### 소통의 부재
- 비상대기엔지니어 : 명확한 소통을 하기 어려운 상황. 동료가 어떤 일을 하고 있는지 아무도 모름. 디버깅이나 이슈의 해결을 도화줄 다른 엔지니어를 효과적으로 활용하지 못함.
- 고객과 비즈니스리더의 분노
### 프리랜서의 고용
- 개발자4는 자신이 최선이라고 생각하는 방향으로 시스템을 변경. 
- 장애조치의 책임을 맡고 있는 엔지니어와 협업을 하지 않음.

## 장애 관리 절차의 기본 요소들

### 책임에 대한 귀납적인 분리
- 장애 조치 참여자들이 자신의 역할을 이해하고 다른 이의 영역을 침범하지 않음. -> 개개인의 작업 자율성 보장
- 계획을 수립하는 리더는 미리 역할을 위임하고 정보를 보고 받음

- 장애조치 역할
	- 장애 제어 (장애 제어자)
		- 장애에 대한 높은 수준의 상태를 확인
		- 장애 조치 팀 구성. 우선순위 선정, 책임분배
		- 운영 업무 방해요소 제거
	- 운영 업무
		- 장애 제어자와 협력하여 실제 업무를 수행하며 장애에 대처
		- 장애 조치 중에는 **오직 운영팀만 시스템을 변경**해야한다.
	- 의사 소통
		- 장애 조치팀의 대외 창구역할 
		- 의사결정자들에게 상황을 보고하며, 장애조치 문서를 최신의 상태로 유지.
	- 계획
		- 운영팀을 도와 버그를 수집. 업무보조. 
		- 시스템 상태변경 추적 후 장애 조치 후 원래대로 복구 하는 등 장기적 대응
	
## 확실한 컨트롤 타워
- 장애 제어자와 명확한 소통이 필요. 
- 같은 공간에 함께 하는 것이 좋다
- 이메일이나 IRC로 상황을 지속적으로 공유

## 실시간 장애 조치 문서 
- 장애 제어자는 실시간으로 장애 조치 문서를 관리해야 함.
- 부록 C 참고
- 문서를 공유문서로 작성하여 동시 편집이 가능하게 한다.
- 포스트모텀 분석을 위해 계속 유지되어야하며, 메타 분석에도 활용할 수 있다.

## 명확하고 즉각적인 업무이관
- 장애 제어자의 일과가 끝나면 다른 담당자에게 이관
- 이관할 장애 제어자에 대해 공지
- 완전히 이관에 대한 인지가 끝날때까지 연락을 끊어서는 안된다.
- 장애를 조치하고 있는 사람들에게도 전달되어 누가 관리하고 있는지 명확히한다.

## 적절하게 관리한 장애 조치
- 오후2시. 여러군데의 데이터 센터가 미친듯한 트래픽으로 터져나가는 중
- 시스템은 모든 요청을 처리할 수 없는 상태
- 1. 장애 관리 절차를 따른다
- 2. 서버개발자 2 에게 장애 제어자 분담. 비상대기엔지니어는 기술업무수행
- 3. 이메일로 관련 수신자들에게 전달 및 장애 영향도 분석 
- 세번째 장애 발생
- 4. 장애제어자는 IRC 디버깅 채널에서 장애알림을 확인하고 이메일 스레드를 업데이트
- 5. 장애제어자는 엔지니어에게 개발자 지원 필요여부를 확인
- 6. 개발자 합류. 장애 조치 문서를 통해 현재 상태를 분석
- 장애제어자는 개발자들에게 업무수행에 대한 내용을 비상대기엔지니어에게 공유할 것을 당부
- 오후5시. 장애제어자의 퇴근시간. 장애 조치문서를 업데이트.
- 컨퍼런스 콜을 통한 상황 공유
- 오후6시. 다른 사무실의 동료들에게 역할을 이관.
- 다음날 출근 장애 조치를 완료하고 포스트모텀을 작성

## 언제 장애를 선언할 것인가
- 장애를 빨리 선언하고 최대한 간결한 해결책을 찾아 장애를 조치
- 다음중 하나라도 해당한다면 이를 장애로 선언한다
	- 문제를 해결하기 위해 다른 팀의 도움이 필요한가?
	- 문제가 사용자에게 영향을 미쳤는가?
	- 문제 발생 이후 한시간 동안 집중적으로 분석했으나 해결되지 않았는가?
	
- 장애 관리 숙련도는 지속적으로 활용하지 않으면 금방 쇠퇴
	- 여러 팀이 참여하는 운영 업무에 공통적으로 활용 가능
	- 이미 해결된 비상대기 이슈에 대해 역할을 실습

